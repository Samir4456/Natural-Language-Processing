{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ada81ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Samir\n",
      "[nltk_data]     Pokharel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f655333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4623\n"
     ]
    }
   ],
   "source": [
    "# usinf news category from brown corpus\n",
    "sentences = brown.sents(categories=\"news\")\n",
    "print(\"Number of sentences:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634909bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sentences: 4608\n",
      "Example sentence tokens: ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place']\n"
     ]
    }
   ],
   "source": [
    "# clean + tokenize(already tokenized in brown corpus just need to normalize)\n",
    "def clean_token(tok: str):\n",
    "    tok = tok.lower()\n",
    "    tok = re.sub(r\"[^a-z']+\", \"\", tok)  # remove non-letters\n",
    "    return tok\n",
    "\n",
    "docs = []\n",
    "for sent in sentences:\n",
    "    cleaned = [clean_token(t) for t in sent]\n",
    "    cleaned = [t for t in cleaned if t]  # drop empty tokens\n",
    "    if len(cleaned) > 0:\n",
    "        docs.append(cleaned)\n",
    "\n",
    "print(\"Processed sentences:\", len(docs))\n",
    "print(\"Example sentence tokens:\", docs[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee6e75a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 87757\n",
      "Unique tokens: 12286\n"
     ]
    }
   ],
   "source": [
    "all_tokens = [w for s in docs for w in s]\n",
    "print(\"Total tokens:\", len(all_tokens))\n",
    "print(\"Unique tokens:\", len(set(all_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc255a3",
   "metadata": {},
   "source": [
    "# vocabulary building + word↔id encoding + a dynamic window (default=2) pair generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d28a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build vocabulary + encode sentences + dynamic window pair generator\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c31524",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5          # drop rare words (helps speed + quality)\n",
    "window_default = 2     # requirement: default window = 2\n",
    "seed = 1337\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7703d06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2490\n",
      "Most common words: [('the', 6386), ('of', 2861), ('and', 2187), ('a', 2170), ('to', 2147), ('in', 2020), ('for', 970), ('that', 829), ('is', 733), ('was', 717)]\n"
     ]
    }
   ],
   "source": [
    "# Build vocab\n",
    "word_counts = Counter([w for sent in docs for w in sent])\n",
    "\n",
    "# Keep words >= min_count\n",
    "vocab = [w for w, c in word_counts.items() if c >= min_count]\n",
    "vocab = sorted(vocab)  # stable ordering\n",
    "\n",
    "# Add special token for unknown words \n",
    "UNK = \"<unk>\"\n",
    "vocab = [UNK] + vocab\n",
    "\n",
    "stoi = {w: i for i, w in enumerate(vocab)}\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Most common words:\", word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d0d6c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example encoded sentence: [2218, 921, 550, 987, 1208, 1931, 913, 96, 1161, 1534, 0, 1830, 1740, 724, 1754, 1506, 774, 2, 2216, 112]\n",
      "Example decoded back: ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', '<unk>', 'recent', 'primary', 'election', 'produced', 'no', 'evidence', \"''\", 'that', 'any']\n"
     ]
    }
   ],
   "source": [
    "# Encode sentences (words -> ids)\n",
    "def encode_sentence(sent):\n",
    "    return [stoi.get(w, stoi[UNK]) for w in sent]\n",
    "\n",
    "encoded_docs = [encode_sentence(sent) for sent in docs]\n",
    "\n",
    "print(\"Example encoded sentence:\", encoded_docs[0][:20])\n",
    "print(\"Example decoded back:\", [itos[i] for i in encoded_docs[0][:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af495c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic window pair generator\n",
    "def generate_skipgram_pairs(encoded_sents, window=window_default, dynamic=True):\n",
    "    \"\"\"\n",
    "    encoded_sents: list[list[int]]\n",
    "    window: max window size\n",
    "    dynamic: if True, random window size per center word in [1, window]\n",
    "    \"\"\"\n",
    "    for sent in encoded_sents:\n",
    "        n = len(sent)\n",
    "        for i, center in enumerate(sent):\n",
    "            w = random.randint(1, window) if dynamic else window\n",
    "            left = max(0, i - w)\n",
    "            right = min(n, i + w + 1)\n",
    "            for j in range(left, right):\n",
    "                if j != i:\n",
    "                    context = sent[j]\n",
    "                    yield center, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f679bbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center: the -> context: fulton\n",
      "center: the -> context: county\n",
      "center: fulton -> context: the\n",
      "center: fulton -> context: county\n",
      "center: county -> context: the\n",
      "center: county -> context: fulton\n",
      "center: county -> context: grand\n",
      "center: county -> context: jury\n",
      "center: grand -> context: fulton\n",
      "center: grand -> context: county\n"
     ]
    }
   ],
   "source": [
    "pair_gen = generate_skipgram_pairs(encoded_docs[:2], window=2, dynamic=True)\n",
    "for _ in range(10):\n",
    "    c, ctx = next(pair_gen)\n",
    "    print(\"center:\", itos[c], \"-> context:\", itos[ctx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c71e6a4",
   "metadata": {},
   "source": [
    "# Skipgram Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a478d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9194936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "emb_dim = 100\n",
    "window = 2              # requirement default=2\n",
    "dynamic_window = True   # requirement: dynamic window support\n",
    "batch_size = 2048\n",
    "steps = 3000            # increase later for better quality (e.g., 20000+)\n",
    "lr = 3e-3\n",
    "\n",
    "# How many sentences to sample from each step (controls speed)\n",
    "sentences_per_step = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48134e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: Skip-gram (full softmax)\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.out = nn.Linear(emb_dim, vocab_size, bias=False)\n",
    "\n",
    "        # Good init\n",
    "        nn.init.normal_(self.in_embed.weight, mean=0.0, std=0.01)\n",
    "\n",
    "    def forward(self, center_ids):\n",
    "        # center_ids: (B,)\n",
    "        v = self.in_embed(center_ids)          # (B, D)\n",
    "        logits = self.out(v)                   # (B, V)\n",
    "        return logits\n",
    "    \n",
    "model = SkipGram(vocab_size, emb_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b134f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: sample a batch of (center, context) pairs\n",
    "def sample_pairs_batch(encoded_sents, batch_size, window=2, dynamic=True, sentences_per_step=64):\n",
    "    # pick some random sentences\n",
    "    chosen = random.sample(encoded_sents, k=min(sentences_per_step, len(encoded_sents)))\n",
    "    gen = generate_skipgram_pairs(chosen, window=window, dynamic=dynamic)\n",
    "\n",
    "    centers = []\n",
    "    contexts = []\n",
    "    # gather pairs until batch is full (or generator ends)\n",
    "    while len(centers) < batch_size:\n",
    "        try:\n",
    "            c, ctx = next(gen)\n",
    "            centers.append(c)\n",
    "            contexts.append(ctx)\n",
    "        except StopIteration:\n",
    "            # if we ran out, re-sample new sentences\n",
    "            chosen = random.sample(encoded_sents, k=min(sentences_per_step, len(encoded_sents)))\n",
    "            gen = generate_skipgram_pairs(chosen, window=window, dynamic=dynamic)\n",
    "\n",
    "    centers = torch.tensor(centers, dtype=torch.long, device=device)\n",
    "    contexts = torch.tensor(contexts, dtype=torch.long, device=device)\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "205b75b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200/3000 | loss 5.6644\n",
      "step 400/3000 | loss 5.3228\n",
      "step 600/3000 | loss 5.0530\n",
      "step 800/3000 | loss 5.4900\n",
      "step 1000/3000 | loss 5.1489\n",
      "step 1200/3000 | loss 5.2842\n",
      "step 1400/3000 | loss 4.9151\n",
      "step 1600/3000 | loss 4.9734\n",
      "step 1800/3000 | loss 5.0238\n",
      "step 2000/3000 | loss 4.7883\n",
      "step 2200/3000 | loss 4.6731\n",
      "step 2400/3000 | loss 4.8176\n",
      "step 2600/3000 | loss 4.8152\n",
      "step 2800/3000 | loss 4.6766\n",
      "step 3000/3000 | loss 4.6450\n",
      "Done training Skip-gram (full softmax).\n",
      "Training time: 8.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "import time\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "# IMPORTANT for GPU timing\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for step in range(1, steps + 1):\n",
    "    center_ids, target_ctx_ids = sample_pairs_batch(\n",
    "        encoded_docs,\n",
    "        batch_size=batch_size,\n",
    "        window=window,\n",
    "        dynamic=dynamic_window,\n",
    "        sentences_per_step=sentences_per_step\n",
    "    )\n",
    "\n",
    "    logits = model(center_ids)  # (B, V)\n",
    "    loss = F.cross_entropy(logits, target_ctx_ids)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(f\"step {step}/{steps} | loss {loss.item():.4f}\")\n",
    "\n",
    "# synchronize again before stopping timer\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "train_time_sec = end_time - start_time\n",
    "\n",
    "print(\"Done training Skip-gram (full softmax).\")\n",
    "print(f\"Training time: {train_time_sec:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cebec361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'Word2Vec Skip-gram (full softmax)',\n",
       " 'window': 2,\n",
       " 'dynamic_window': True,\n",
       " 'embedding_dim': 100,\n",
       " 'batch_size': 2048,\n",
       " 'steps': 3000,\n",
       " 'training_time_sec': 8.647381499999995,\n",
       " 'final_loss': 4.644984245300293}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics dictionary \n",
    "skipgram_metrics = {\n",
    "    \"model\": \"Word2Vec Skip-gram (full softmax)\",\n",
    "    \"window\": window,\n",
    "    \"dynamic_window\": dynamic_window,\n",
    "    \"embedding_dim\": emb_dim,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"steps\": steps,\n",
    "    \"training_time_sec\": train_time_sec,\n",
    "    \"final_loss\": loss.item()\n",
    "}\n",
    "\n",
    "skipgram_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b1b34",
   "metadata": {},
   "source": [
    "# Evaluate trained embeddings (nearest neighbors + similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4e2e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get embedding matrix (V, D)\n",
    "with torch.no_grad():\n",
    "    W = model.in_embed.weight.detach().to(\"cpu\")  # move to CPU for easy use\n",
    "\n",
    "# Normalize once for cosine similarity\n",
    "W_norm = F.normalize(W, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f46e66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def most_similar(word, topk=10):\n",
    "    if word not in stoi:\n",
    "        print(f\"'{word}' not in vocab. Try another word.\")\n",
    "        return []\n",
    "    wid = stoi[word]\n",
    "    query = W_norm[wid]  # (D,)\n",
    "\n",
    "    # cosine sim with all words\n",
    "    sims = torch.mv(W_norm, query)  # (V,)\n",
    "    sims[wid] = -1e9  # exclude itself\n",
    "\n",
    "    vals, idxs = torch.topk(sims, k=topk)\n",
    "    results = [(itos[i.item()], vals[j].item()) for j, i in enumerate(idxs)]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43c14c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar to 'market':\n",
      "  textile         0.491\n",
      "  underwater      0.480\n",
      "  enjoyed         0.467\n",
      "  stock           0.423\n",
      "  value           0.416\n",
      "  exchange        0.415\n",
      "  power           0.413\n",
      "  wall            0.412\n",
      "\n",
      "Most similar to 'bank':\n",
      "  grand           0.503\n",
      "  honors          0.491\n",
      "  chester         0.488\n",
      "  establishment   0.479\n",
      "  freedom         0.476\n",
      "  boston          0.466\n",
      "  monthly         0.453\n",
      "  lodge           0.445\n",
      "\n",
      "Most similar to 'oil':\n",
      "  teamsters       0.528\n",
      "  electric        0.524\n",
      "  katanga         0.510\n",
      "  cotton          0.497\n",
      "  palm            0.496\n",
      "  transportation  0.496\n",
      "  hollywood       0.488\n",
      "  minister        0.483\n",
      "\n",
      "Most similar to 'government':\n",
      "  situation       0.461\n",
      "  prowestern      0.460\n",
      "  measure         0.453\n",
      "  patient         0.442\n",
      "  coalition       0.437\n",
      "  appeals         0.431\n",
      "  ordinance       0.427\n",
      "  united          0.424\n",
      "\n",
      "Most similar to 'war':\n",
      "  cold            0.484\n",
      "  power           0.460\n",
      "  camera          0.459\n",
      "  changed         0.451\n",
      "  competition     0.446\n",
      "  bible           0.438\n",
      "  free            0.432\n",
      "  constitutional  0.425\n",
      "\n",
      "Most similar to 'money':\n",
      "  best            0.453\n",
      "  raise           0.449\n",
      "  information     0.444\n",
      "  snow            0.443\n",
      "  signatures      0.430\n",
      "  view            0.418\n",
      "  crew            0.402\n",
      "  theme           0.398\n"
     ]
    }
   ],
   "source": [
    "def similarity(w1, w2):\n",
    "    if w1 not in stoi or w2 not in stoi:\n",
    "        print(\"One of the words is not in vocab.\")\n",
    "        return None\n",
    "    v1 = W_norm[stoi[w1]]\n",
    "    v2 = W_norm[stoi[w2]]\n",
    "    return float(torch.dot(v1, v2))\n",
    "\n",
    "# ---- Try a few words (edit these to whatever you want) ----\n",
    "test_words = [\"market\", \"bank\", \"oil\", \"government\", \"war\", \"money\"]\n",
    "\n",
    "for w in test_words:\n",
    "    if w in stoi:\n",
    "        print(f\"\\nMost similar to '{w}':\")\n",
    "        for ww, s in most_similar(w, topk=8):\n",
    "            print(f\"  {ww:15s} {s:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df53f124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity examples:\n",
      "sim(money, bank) = 0.288\n",
      "sim(oil, market) = 0.196\n",
      "sim(war, government) = 0.249\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSimilarity examples:\")\n",
    "pairs = [(\"money\", \"bank\"), (\"oil\", \"market\"), (\"war\", \"government\")]\n",
    "for a, b in pairs:\n",
    "    if a in stoi and b in stoi:\n",
    "        print(f\"sim({a}, {b}) = {similarity(a,b):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec497c6e",
   "metadata": {},
   "source": [
    "# Word2Vec Skip-gram NEG (Negative Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8248a0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "step 500/8000 | loss 1.0873\n",
      "step 1000/8000 | loss 0.9989\n",
      "step 1500/8000 | loss 0.9317\n",
      "step 2000/8000 | loss 0.8890\n",
      "step 2500/8000 | loss 0.8651\n",
      "step 3000/8000 | loss 0.8544\n",
      "step 3500/8000 | loss 0.8275\n",
      "step 4000/8000 | loss 0.8436\n",
      "step 4500/8000 | loss 0.8355\n",
      "step 5000/8000 | loss 0.8460\n",
      "step 5500/8000 | loss 0.8296\n",
      "step 6000/8000 | loss 0.8076\n",
      "step 6500/8000 | loss 0.8160\n",
      "step 7000/8000 | loss 0.7875\n",
      "step 7500/8000 | loss 0.8186\n",
      "step 8000/8000 | loss 0.7966\n",
      "Done training Skip-gram NEG.\n",
      "Training time: 70.81 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'Word2Vec Skip-gram NEG',\n",
       " 'window': 2,\n",
       " 'dynamic_window': True,\n",
       " 'embedding_dim': 100,\n",
       " 'batch_size': 4096,\n",
       " 'steps': 8000,\n",
       " 'num_negatives': 10,\n",
       " 'training_time_sec': 70.8091306,\n",
       " 'final_loss': 0.7965717315673828}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "emb_dim = 100\n",
    "window = 2\n",
    "dynamic_window = True\n",
    "batch_size = 4096\n",
    "steps = 8000           # NEG usually needs more steps than full softmax\n",
    "lr = 3e-3\n",
    "\n",
    "sentences_per_step = 64\n",
    "num_negatives = 10     # K negatives per positive\n",
    "\n",
    "\n",
    "# Build negative sampling distribution (unigram^0.75)\n",
    "\n",
    "# Count words in encoded_docs (ids)\n",
    "id_counts = Counter([wid for sent in encoded_docs for wid in sent])\n",
    "\n",
    "counts = np.zeros(vocab_size, dtype=np.float64)\n",
    "for wid, c in id_counts.items():\n",
    "    counts[wid] = c\n",
    "\n",
    "# avoid issues if vocab[0] = <unk> has low count\n",
    "counts = np.maximum(counts, 1.0)\n",
    "\n",
    "unigram = counts ** 0.75\n",
    "neg_probs = unigram / unigram.sum()\n",
    "\n",
    "# We'll sample negatives with torch.multinomial using probs tensor on GPU\n",
    "neg_probs_t = torch.tensor(neg_probs, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "# Model: two embedding tables (input + output)\n",
    "\n",
    "class SkipGramNEG(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        nn.init.normal_(self.in_embed.weight, mean=0.0, std=0.01)\n",
    "        nn.init.normal_(self.out_embed.weight, mean=0.0, std=0.01)\n",
    "\n",
    "    def forward(self, center_ids, pos_ctx_ids, neg_ctx_ids):\n",
    "        \"\"\"\n",
    "        center_ids: (B,)\n",
    "        pos_ctx_ids: (B,)\n",
    "        neg_ctx_ids: (B, K)\n",
    "        \"\"\"\n",
    "        v = self.in_embed(center_ids)               # (B, D)\n",
    "        u_pos = self.out_embed(pos_ctx_ids)         # (B, D)\n",
    "        u_neg = self.out_embed(neg_ctx_ids)         # (B, K, D)\n",
    "\n",
    "        # Positive score: v dot u_pos -> (B,)\n",
    "        pos_score = torch.sum(v * u_pos, dim=1)\n",
    "\n",
    "        # Negative scores: v dot u_neg -> (B, K)\n",
    "        neg_score = torch.bmm(u_neg, v.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        # Loss:\n",
    "        # maximize log(sigmoid(pos)) + sum log(sigmoid(-neg))\n",
    "        loss_pos = F.logsigmoid(pos_score).mean()\n",
    "        loss_neg = F.logsigmoid(-neg_score).mean()\n",
    "        loss = -(loss_pos + loss_neg)\n",
    "        return loss\n",
    "\n",
    "model_neg = SkipGramNEG(vocab_size, emb_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model_neg.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Batch sampler: positives + negatives\n",
    "\n",
    "def sample_pos_batch(encoded_sents, batch_size, window=2, dynamic=True, sentences_per_step=64):\n",
    "    chosen = random.sample(encoded_sents, k=min(sentences_per_step, len(encoded_sents)))\n",
    "    gen = generate_skipgram_pairs(chosen, window=window, dynamic=dynamic)\n",
    "\n",
    "    centers, pos_ctx = [], []\n",
    "    while len(centers) < batch_size:\n",
    "        try:\n",
    "            c, ctx = next(gen)\n",
    "            centers.append(c)\n",
    "            pos_ctx.append(ctx)\n",
    "        except StopIteration:\n",
    "            chosen = random.sample(encoded_sents, k=min(sentences_per_step, len(encoded_sents)))\n",
    "            gen = generate_skipgram_pairs(chosen, window=window, dynamic=dynamic)\n",
    "\n",
    "    centers = torch.tensor(centers, dtype=torch.long, device=device)\n",
    "    pos_ctx = torch.tensor(pos_ctx, dtype=torch.long, device=device)\n",
    "    return centers, pos_ctx\n",
    "\n",
    "def sample_negatives(batch_size, K):\n",
    "    # returns (B, K)\n",
    "    return torch.multinomial(neg_probs_t, num_samples=batch_size * K, replacement=True).view(batch_size, K)\n",
    "\n",
    "\n",
    "# Train loop with timing\n",
    "\n",
    "model_neg.train()\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for step in range(1, steps + 1):\n",
    "    center_ids, pos_ctx_ids = sample_pos_batch(\n",
    "        encoded_docs,\n",
    "        batch_size=batch_size,\n",
    "        window=window,\n",
    "        dynamic=dynamic_window,\n",
    "        sentences_per_step=sentences_per_step\n",
    "    )\n",
    "    neg_ctx_ids = sample_negatives(batch_size, num_negatives)\n",
    "\n",
    "    loss = model_neg(center_ids, pos_ctx_ids, neg_ctx_ids)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step {step}/{steps} | loss {loss.item():.4f}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "train_time_sec = time.perf_counter() - start_time\n",
    "\n",
    "print(\"Done training Skip-gram NEG.\")\n",
    "print(f\"Training time: {train_time_sec:.2f} seconds\")\n",
    "\n",
    "# Save metrics for Task 2 table\n",
    "skipgram_neg_metrics = {\n",
    "    \"model\": \"Word2Vec Skip-gram NEG\",\n",
    "    \"window\": window,\n",
    "    \"dynamic_window\": dynamic_window,\n",
    "    \"embedding_dim\": emb_dim,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"steps\": steps,\n",
    "    \"num_negatives\": num_negatives,\n",
    "    \"training_time_sec\": train_time_sec,\n",
    "    \"final_loss\": loss.item()\n",
    "}\n",
    "\n",
    "skipgram_neg_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2338a",
   "metadata": {},
   "source": [
    "# Build GloVe co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1129adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence pairs: 86927\n",
      "Build time: 0.32 seconds\n",
      "X(the, fulton) = 6.500\n",
      "X(fulton, the) = 6.000\n",
      "X(fulton, county) = 6.000\n",
      "X(county, fulton) = 6.000\n",
      "X(county, grand) = 1.000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "# GloVe co-occurrence builder\n",
    "\n",
    "window = 2\n",
    "dynamic_window = True\n",
    "\n",
    "# You can cap vocab if you want faster GloVe (optional)\n",
    "# max_vocab = 20000\n",
    "# (for now we'll keep full vocab)\n",
    "\n",
    "def build_cooccurrence(encoded_sents, window=2, dynamic=True):\n",
    "    \"\"\"\n",
    "    Builds sparse co-occurrence counts X(i,j) in a dict.\n",
    "    Uses distance weighting: 1/distance (common in GloVe setups).\n",
    "    \"\"\"\n",
    "    X = defaultdict(float)\n",
    "\n",
    "    for sent in encoded_sents:\n",
    "        n = len(sent)\n",
    "        for i, wi in enumerate(sent):\n",
    "            w = random.randint(1, window) if dynamic else window\n",
    "            left = max(0, i - w)\n",
    "            right = min(n, i + w + 1)\n",
    "\n",
    "            for j in range(left, right):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                wj = sent[j]\n",
    "                dist = abs(i - j)\n",
    "                X[(wi, wj)] += 1.0 / dist  # distance weighting\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "# Build co-occurrence + time it\n",
    "\n",
    "start = time.perf_counter()\n",
    "X = build_cooccurrence(encoded_docs, window=window, dynamic=dynamic_window)\n",
    "cooc_time_sec = time.perf_counter() - start\n",
    "\n",
    "print(\"Co-occurrence pairs:\", len(X))\n",
    "print(f\"Build time: {cooc_time_sec:.2f} seconds\")\n",
    "\n",
    "# Show a few example entries\n",
    "for k in list(X.keys())[:5]:\n",
    "    i, j = k\n",
    "    print(f\"X({itos[i]}, {itos[j]}) = {X[k]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca05cc",
   "metadata": {},
   "source": [
    "# Train GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f598c5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training pairs after filtering: 86927\n",
      "epoch 1/5 | avg loss 0.0864\n",
      "epoch 2/5 | avg loss 0.0430\n",
      "epoch 3/5 | avg loss 0.0354\n",
      "epoch 4/5 | avg loss 0.0300\n",
      "epoch 5/5 | avg loss 0.0259\n",
      "Done training GloVe.\n",
      "Training time: 0.92 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'GloVe',\n",
       " 'window': 2,\n",
       " 'dynamic_window': True,\n",
       " 'embedding_dim': 100,\n",
       " 'batch_size': 4096,\n",
       " 'epochs': 5,\n",
       " 'cooc_build_time_sec': 0.3200208000000089,\n",
       " 'training_time_sec': 0.9200410000000261,\n",
       " 'final_loss': 0.0455159991979599,\n",
       " 'num_cooc_pairs': 86927,\n",
       " 'num_train_pairs': 86927}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# Convert sparse X dict -> tensors\n",
    "\n",
    "# Each entry: (i, j, x_ij)\n",
    "ijs = []\n",
    "xijs = []\n",
    "for (i, j), v in X.items():\n",
    "    # Optional: drop very tiny counts (speeds up)\n",
    "    if v >= 0.1:\n",
    "        ijs.append((i, j))\n",
    "        xijs.append(v)\n",
    "\n",
    "ijs = torch.tensor(ijs, dtype=torch.long)\n",
    "xijs = torch.tensor(xijs, dtype=torch.float32)\n",
    "\n",
    "print(\"Training pairs after filtering:\", len(xijs))\n",
    "\n",
    "# Move to device\n",
    "ijs = ijs.to(device)\n",
    "xijs = xijs.to(device)\n",
    "\n",
    "\n",
    "# GloVe hyperparameters\n",
    "\n",
    "emb_dim = 100\n",
    "epochs = 5              # raise later (e.g., 20-50) for better embeddings\n",
    "lr = 0.05\n",
    "batch_size = 4096\n",
    "\n",
    "x_max = 100.0\n",
    "alpha = 0.75\n",
    "\n",
    "\n",
    "# GloVe model\n",
    "\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.wi = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.wj = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.bi = nn.Embedding(vocab_size, 1)\n",
    "        self.bj = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "        nn.init.normal_(self.wi.weight, mean=0.0, std=0.01)\n",
    "        nn.init.normal_(self.wj.weight, mean=0.0, std=0.01)\n",
    "        nn.init.zeros_(self.bi.weight)\n",
    "        nn.init.zeros_(self.bj.weight)\n",
    "\n",
    "    def forward(self, i_ids, j_ids, x_ij):\n",
    "        vi = self.wi(i_ids)                 # (B, D)\n",
    "        vj = self.wj(j_ids)                 # (B, D)\n",
    "        bi = self.bi(i_ids).squeeze(1)      # (B,)\n",
    "        bj = self.bj(j_ids).squeeze(1)      # (B,)\n",
    "\n",
    "        # weight function f(x)\n",
    "        # f(x) = (x/x_max)^alpha if x < x_max else 1\n",
    "        fx = torch.where(x_ij < x_max, (x_ij / x_max) ** alpha, torch.ones_like(x_ij))\n",
    "\n",
    "        # GloVe loss: f(x) * (vi·vj + bi + bj - log(x))^2\n",
    "        pred = (vi * vj).sum(dim=1) + bi + bj\n",
    "        logx = torch.log(x_ij)\n",
    "        loss = (fx * (pred - logx) ** 2).mean()\n",
    "        return loss\n",
    "\n",
    "glove = GloVe(vocab_size, emb_dim).to(device)\n",
    "opt = torch.optim.AdamW(glove.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Training loop with timing\n",
    "\n",
    "num_samples = xijs.size(0)\n",
    "num_batches = math.ceil(num_samples / batch_size)\n",
    "\n",
    "def batch_iter():\n",
    "    # shuffle indices each epoch\n",
    "    idx = torch.randperm(num_samples, device=device)\n",
    "    for b in range(num_batches):\n",
    "        batch_idx = idx[b * batch_size : (b + 1) * batch_size]\n",
    "        ij = ijs[batch_idx]\n",
    "        x = xijs[batch_idx]\n",
    "        yield ij[:, 0], ij[:, 1], x\n",
    "\n",
    "glove.train()\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "last_loss = None\n",
    "for ep in range(1, epochs + 1):\n",
    "    ep_loss = 0.0\n",
    "    for i_ids, j_ids, x_ij in batch_iter():\n",
    "        loss = glove(i_ids, j_ids, x_ij)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        ep_loss += loss.item()\n",
    "        last_loss = loss.item()\n",
    "\n",
    "    print(f\"epoch {ep}/{epochs} | avg loss {ep_loss/num_batches:.4f}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "train_time_sec = time.perf_counter() - start_time\n",
    "\n",
    "print(\"Done training GloVe.\")\n",
    "print(f\"Training time: {train_time_sec:.2f} seconds\")\n",
    "\n",
    "# Final embedding = wi + wj (common choice)\n",
    "with torch.no_grad():\n",
    "    glove_W = (glove.wi.weight + glove.wj.weight).detach().to(\"cpu\")\n",
    "    glove_W_norm = F.normalize(glove_W, p=2, dim=1)\n",
    "\n",
    "# Save metrics for Task 2 table\n",
    "glove_metrics = {\n",
    "    \"model\": \"GloVe\",\n",
    "    \"window\": window,\n",
    "    \"dynamic_window\": dynamic_window,\n",
    "    \"embedding_dim\": emb_dim,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    \"cooc_build_time_sec\": cooc_time_sec,\n",
    "    \"training_time_sec\": train_time_sec,\n",
    "    \"final_loss\": float(last_loss) if last_loss is not None else None,\n",
    "    \"num_cooc_pairs\": int(len(X)),\n",
    "    \"num_train_pairs\": int(num_samples),\n",
    "}\n",
    "\n",
    "glove_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af477a7",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a6ef2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>window</th>\n",
       "      <th>dynamic_window</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>steps</th>\n",
       "      <th>epochs</th>\n",
       "      <th>num_negatives</th>\n",
       "      <th>cooc_build_time_sec</th>\n",
       "      <th>training_time_sec</th>\n",
       "      <th>final_loss</th>\n",
       "      <th>num_cooc_pairs</th>\n",
       "      <th>num_train_pairs</th>\n",
       "      <th>total_time_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word2Vec Skip-gram (full softmax)</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>2048</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.65</td>\n",
       "      <td>4.6450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word2Vec Skip-gram NEG</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>4096</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.81</td>\n",
       "      <td>0.7966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>86927.0</td>\n",
       "      <td>86927.0</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               model  window  dynamic_window  embedding_dim  \\\n",
       "0  Word2Vec Skip-gram (full softmax)       2            True            100   \n",
       "1             Word2Vec Skip-gram NEG       2            True            100   \n",
       "2                              GloVe       2            True            100   \n",
       "\n",
       "   batch_size   steps  epochs  num_negatives  cooc_build_time_sec  \\\n",
       "0        2048  3000.0     NaN            NaN                  NaN   \n",
       "1        4096  8000.0     NaN           10.0                  NaN   \n",
       "2        4096     NaN     5.0            NaN                 0.32   \n",
       "\n",
       "   training_time_sec  final_loss  num_cooc_pairs  num_train_pairs  \\\n",
       "0               8.65      4.6450             NaN              NaN   \n",
       "1              70.81      0.7966             NaN              NaN   \n",
       "2               0.92      0.0455         86927.0          86927.0   \n",
       "\n",
       "   total_time_sec  \n",
       "0            8.65  \n",
       "1           70.81  \n",
       "2            1.24  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 2 - Step 1: Build comparison table + export CSV\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Put all metrics dicts into a list (only include ones that exist)\n",
    "rows = []\n",
    "for d in [\"skipgram_metrics\", \"skipgram_neg_metrics\", \"glove_metrics\"]:\n",
    "    if d in globals():\n",
    "        rows.append(globals()[d])\n",
    "    else:\n",
    "        print(f\"Warning: {d} not found (did you run that model cell?)\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Reorder columns nicely (keep only columns that exist)\n",
    "preferred_cols = [\n",
    "    \"model\",\n",
    "    \"window\",\n",
    "    \"dynamic_window\",\n",
    "    \"embedding_dim\",\n",
    "    \"batch_size\",\n",
    "    \"steps\",\n",
    "    \"epochs\",\n",
    "    \"num_negatives\",\n",
    "    \"cooc_build_time_sec\",\n",
    "    \"training_time_sec\",\n",
    "    \"final_loss\",\n",
    "    \"num_cooc_pairs\",\n",
    "    \"num_train_pairs\",\n",
    "]\n",
    "df = df[[c for c in preferred_cols if c in df.columns]]\n",
    "\n",
    "# Add a total-time column (useful for comparing GloVe fairly)\n",
    "if \"cooc_build_time_sec\" in df.columns:\n",
    "    df[\"total_time_sec\"] = df.get(\"cooc_build_time_sec\", 0).fillna(0) + df.get(\"training_time_sec\", 0).fillna(0)\n",
    "else:\n",
    "    df[\"total_time_sec\"] = df.get(\"training_time_sec\", 0).fillna(0)\n",
    "\n",
    "# Make times easier to read\n",
    "for c in [\"cooc_build_time_sec\", \"training_time_sec\", \"total_time_sec\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(float).round(2)\n",
    "\n",
    "if \"final_loss\" in df.columns:\n",
    "    df[\"final_loss\"] = df[\"final_loss\"].astype(float).round(4)\n",
    "\n",
    "# Show the table\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4084c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: task2_metrics_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Save as CSV for your report / submission\n",
    "df.to_csv(\"task2_metrics_comparison.csv\", index=False)\n",
    "print(\"Saved: task2_metrics_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0833cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Window</th>\n",
       "      <th>Dim</th>\n",
       "      <th>Train Time (s)</th>\n",
       "      <th>Total Time (s)</th>\n",
       "      <th>Final Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word2Vec Skip-gram (full softmax)</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>8.65</td>\n",
       "      <td>8.65</td>\n",
       "      <td>4.6450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word2Vec Skip-gram NEG</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>70.81</td>\n",
       "      <td>70.81</td>\n",
       "      <td>0.7966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.0455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model  Window  Dim  Train Time (s)  \\\n",
       "0  Word2Vec Skip-gram (full softmax)       2  100            8.65   \n",
       "1             Word2Vec Skip-gram NEG       2  100           70.81   \n",
       "2                              GloVe       2  100            0.92   \n",
       "\n",
       "   Total Time (s)  Final Loss  \n",
       "0            8.65      4.6450  \n",
       "1           70.81      0.7966  \n",
       "2            1.24      0.0455  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 2 - Step 2A: Clean report-style Markdown table\n",
    "\n",
    "report_cols = [\n",
    "    \"model\",\n",
    "    \"window\",\n",
    "    \"embedding_dim\",\n",
    "    \"training_time_sec\",\n",
    "    \"total_time_sec\",\n",
    "    \"final_loss\",\n",
    "]\n",
    "\n",
    "report_cols = [c for c in report_cols if c in df.columns]\n",
    "report_df = df[report_cols].copy()\n",
    "\n",
    "# Rename columns for readability\n",
    "report_df = report_df.rename(columns={\n",
    "    \"model\": \"Model\",\n",
    "    \"window\": \"Window\",\n",
    "    \"embedding_dim\": \"Dim\",\n",
    "    \"training_time_sec\": \"Train Time (s)\",\n",
    "    \"total_time_sec\": \"Total Time (s)\",\n",
    "    \"final_loss\": \"Final Loss\",\n",
    "})\n",
    "\n",
    "report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7c4e13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fastest_model': 'GloVe',\n",
       " 'best_loss_model': 'GloVe',\n",
       " 'slowest_total_model': 'Word2Vec Skip-gram NEG'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 2 - Step 2C: Automatic analysis helpers\n",
    "\n",
    "analysis = {}\n",
    "\n",
    "# Fastest training\n",
    "analysis[\"fastest_model\"] = df.loc[df[\"training_time_sec\"].idxmin(), \"model\"]\n",
    "\n",
    "# Best (lowest) loss\n",
    "analysis[\"best_loss_model\"] = df.loc[df[\"final_loss\"].idxmin(), \"model\"]\n",
    "\n",
    "# Most expensive total time\n",
    "analysis[\"slowest_total_model\"] = df.loc[df[\"total_time_sec\"].idxmax(), \"model\"]\n",
    "\n",
    "analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16698a",
   "metadata": {},
   "source": [
    "# Word Analogy Accuracy (semantic + syntactic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baca6183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: word-test.v1.txt\n",
      "Loading GloVe (gensim) glove-wiki-gigaword-100 ...\n",
      "Loaded gensim model vocab: 400000\n",
      "capital-common-countries questions: 506\n",
      "gram8-past-tense questions: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>syntactic_acc_%</th>\n",
       "      <th>semantic_acc_%</th>\n",
       "      <th>syntactic_total</th>\n",
       "      <th>semantic_total</th>\n",
       "      <th>syntactic_skipped</th>\n",
       "      <th>semantic_skipped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skipgram</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skipgram (NEG)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GloVe (gensim)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.87</td>\n",
       "      <td>0</td>\n",
       "      <td>506</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  syntactic_acc_%  semantic_acc_%  syntactic_total  \\\n",
       "0        Skipgram              0.0            0.00                0   \n",
       "1  Skipgram (NEG)              0.0            5.00                0   \n",
       "2           GloVe              0.0            0.00                0   \n",
       "3  GloVe (gensim)              0.0           93.87                0   \n",
       "\n",
       "   semantic_total  syntactic_skipped  semantic_skipped  \n",
       "0              20                  0               486  \n",
       "1              20                  0               486  \n",
       "2              20                  0               486  \n",
       "3             506                  0                 0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Download Mikolov analogy dataset (word-test.v1.txt)\n",
    "\n",
    "analogy_url = \"http://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt\"\n",
    "analogy_path = \"word-test.v1.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(analogy_url, analogy_path)\n",
    "print(\"Downloaded:\", analogy_path)\n",
    "\n",
    "\n",
    "# Helper: build an embedding accessor for YOUR models\n",
    "\n",
    "def build_embed_lookup_from_torch(itos, stoi, weight_tensor_cpu):\n",
    "    \"\"\"\n",
    "    weight_tensor_cpu: torch.Tensor on CPU, shape (V, D)\n",
    "    returns: dict-like accessor with:\n",
    "      - vectors (torch tensor normalized)\n",
    "      - stoi/itos\n",
    "    \"\"\"\n",
    "    W = weight_tensor_cpu.float()\n",
    "    Wn = F.normalize(W, p=2, dim=1)\n",
    "    return {\"stoi\": stoi, \"itos\": itos, \"W\": W, \"Wn\": Wn}\n",
    "\n",
    "# Skipgram full-softmax embeddings\n",
    "skipgram_lookup = build_embed_lookup_from_torch(\n",
    "    itos, stoi,\n",
    "    model.in_embed.weight.detach().cpu()\n",
    ")\n",
    "\n",
    "# Skipgram NEG embeddings\n",
    "skipgram_neg_lookup = build_embed_lookup_from_torch(\n",
    "    itos, stoi,\n",
    "    model_neg.in_embed.weight.detach().cpu()\n",
    ")\n",
    "\n",
    "# GloVe embeddings (we created glove_W_norm earlier)\n",
    "# glove_W is (V,D) on CPU from Step 7\n",
    "glove_lookup = build_embed_lookup_from_torch(\n",
    "    itos, stoi,\n",
    "    glove_W  # from your GloVe training cell\n",
    ")\n",
    "\n",
    "\n",
    "# Load GloVe (gensim) baseline (optional but requested)\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(\"Loading GloVe (gensim) glove-wiki-gigaword-100 ...\")\n",
    "glove_gensim = api.load(\"glove-wiki-gigaword-100\")  # 100d, 400K vocab :contentReference[oaicite:3]{index=3}\n",
    "print(\"Loaded gensim model vocab:\", len(glove_gensim))\n",
    "\n",
    "\n",
    "# Parse analogy file sections\n",
    "# Only use:\n",
    "#  - semantic: : capital-common-countries\n",
    "#  - syntactic: : gram8-past-tense\n",
    "\n",
    "def load_analogy_questions(path, wanted_sections):\n",
    "    data = {sec: [] for sec in wanted_sections}\n",
    "    current = None\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\":\"):\n",
    "                name = line[1:].strip()\n",
    "                current = name if name in wanted_sections else None\n",
    "                continue\n",
    "            if current is None:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) == 4:\n",
    "                a, b, c, d = parts\n",
    "                data[current].append((a.lower(), b.lower(), c.lower(), d.lower()))\n",
    "    return data\n",
    "\n",
    "wanted = [\"capital-common-countries\", \"gram8-past-tense\"]\n",
    "analogy_data = load_analogy_questions(analogy_path, wanted)\n",
    "for sec in wanted:\n",
    "    print(sec, \"questions:\", len(analogy_data[sec]))\n",
    "\n",
    "\n",
    "# Analogy evaluation:\n",
    "# 3CosAdd: argmax cos( b - a + c, w )\n",
    "# using normalized vectors\n",
    "# report accuracy on questions where all words are in vocab\n",
    "\n",
    "def analogy_accuracy_torch(lookup, questions, topk=1):\n",
    "    stoi_local = lookup[\"stoi\"]\n",
    "    Wn = lookup[\"Wn\"]  # (V,D), normalized\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for a, b, c, d in questions:\n",
    "        if (a not in stoi_local) or (b not in stoi_local) or (c not in stoi_local) or (d not in stoi_local):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        va = Wn[stoi_local[a]]\n",
    "        vb = Wn[stoi_local[b]]\n",
    "        vc = Wn[stoi_local[c]]\n",
    "\n",
    "        query = F.normalize(vb - va + vc, p=2, dim=0)\n",
    "\n",
    "        sims = torch.mv(Wn, query)  # (V,)\n",
    "        # exclude input words from being predicted\n",
    "        sims[stoi_local[a]] = -1e9\n",
    "        sims[stoi_local[b]] = -1e9\n",
    "        sims[stoi_local[c]] = -1e9\n",
    "\n",
    "        pred = torch.argmax(sims).item()\n",
    "        pred_word = lookup[\"itos\"][pred]\n",
    "\n",
    "        total += 1\n",
    "        if pred_word == d:\n",
    "            correct += 1\n",
    "\n",
    "    acc = (correct / total) if total > 0 else 0.0\n",
    "    return acc, total, skipped\n",
    "\n",
    "def analogy_accuracy_gensim(model, questions):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    skipped = 0\n",
    "    for a, b, c, d in questions:\n",
    "        if any(w not in model for w in [a, b, c, d]):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        # most_similar(positive=[b,c], negative=[a])\n",
    "        pred_word = model.most_similar(positive=[b, c], negative=[a], topn=1)[0][0]\n",
    "        total += 1\n",
    "        if pred_word == d:\n",
    "            correct += 1\n",
    "    acc = (correct / total) if total > 0 else 0.0\n",
    "    return acc, total, skipped\n",
    "\n",
    "\n",
    "# Evaluate for the two required sections\n",
    "\n",
    "results_analogy = []\n",
    "\n",
    "# Semantic: capital-common-countries\n",
    "semantic_q = analogy_data[\"capital-common-countries\"]\n",
    "\n",
    "# Syntactic: gram8-past-tense\n",
    "syntactic_q = analogy_data[\"gram8-past-tense\"]\n",
    "\n",
    "for name, lookup in [\n",
    "    (\"Skipgram\", skipgram_lookup),\n",
    "    (\"Skipgram (NEG)\", skipgram_neg_lookup),\n",
    "    (\"GloVe\", glove_lookup),\n",
    "]:\n",
    "    sem_acc, sem_total, sem_skipped = analogy_accuracy_torch(lookup, semantic_q)\n",
    "    syn_acc, syn_total, syn_skipped = analogy_accuracy_torch(lookup, syntactic_q)\n",
    "\n",
    "    results_analogy.append({\n",
    "        \"model\": name,\n",
    "        \"semantic_acc\": sem_acc,\n",
    "        \"semantic_total\": sem_total,\n",
    "        \"semantic_skipped\": sem_skipped,\n",
    "        \"syntactic_acc\": syn_acc,\n",
    "        \"syntactic_total\": syn_total,\n",
    "        \"syntactic_skipped\": syn_skipped,\n",
    "    })\n",
    "\n",
    "# gensim GloVe baseline\n",
    "sem_acc, sem_total, sem_skipped = analogy_accuracy_gensim(glove_gensim, semantic_q)\n",
    "syn_acc, syn_total, syn_skipped = analogy_accuracy_gensim(glove_gensim, syntactic_q)\n",
    "\n",
    "results_analogy.append({\n",
    "    \"model\": \"GloVe (gensim)\",\n",
    "    \"semantic_acc\": sem_acc,\n",
    "    \"semantic_total\": sem_total,\n",
    "    \"semantic_skipped\": sem_skipped,\n",
    "    \"syntactic_acc\": syn_acc,\n",
    "    \"syntactic_total\": syn_total,\n",
    "    \"syntactic_skipped\": syn_skipped,\n",
    "})\n",
    "\n",
    "import pandas as pd\n",
    "analogy_df = pd.DataFrame(results_analogy)\n",
    "\n",
    "# Convert to % for display\n",
    "analogy_df[\"semantic_acc_%\"] = (analogy_df[\"semantic_acc\"] * 100).round(2)\n",
    "analogy_df[\"syntactic_acc_%\"] = (analogy_df[\"syntactic_acc\"] * 100).round(2)\n",
    "\n",
    "analogy_df[[\"model\", \"syntactic_acc_%\", \"semantic_acc_%\", \"syntactic_total\", \"semantic_total\", \"syntactic_skipped\", \"semantic_skipped\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723cf8e6",
   "metadata": {},
   "source": [
    "# WordSim353 similarity correlation (Spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90f6a691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: c:\\Users\\Samir Pokharel\\OneDrive\\Desktop\\Natural Language Processing\\.venv\\lib\\site-packages\\gensim\\test\\test_data\\wordsim353.tsv\n",
      "Final WordSim shape: (354, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#</td>\n",
       "      <td>word</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Word 1 Word 2  Human (mean)\n",
       "1      #   word          1.00\n",
       "2   love    sex          6.77\n",
       "3  tiger    cat          7.35\n",
       "4  tiger  tiger         10.00\n",
       "5   book  paper          7.46"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "ws_path = datapath(\"wordsim353.tsv\")\n",
    "print(\"Using:\", ws_path)\n",
    "\n",
    "# Read as raw text lines (most robust)\n",
    "with open(ws_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    lines = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "rows = []\n",
    "for ln in lines:\n",
    "    # split on whitespace or tabs\n",
    "    parts = ln.split()\n",
    "    if len(parts) < 3:\n",
    "        continue\n",
    "    w1, w2, score = parts[0], parts[1], parts[2]\n",
    "    # skip header-like rows\n",
    "    if w1.lower() in {\"word1\", \"word_1\"} or score.lower() in {\"score\", \"similarity\"}:\n",
    "        continue\n",
    "    rows.append((w1.lower(), w2.lower(), score))\n",
    "\n",
    "ws = pd.DataFrame(rows, columns=[\"Word 1\", \"Word 2\", \"Human (mean)\"])\n",
    "ws[\"Human (mean)\"] = pd.to_numeric(ws[\"Human (mean)\"], errors=\"coerce\")\n",
    "ws = ws.dropna()\n",
    "\n",
    "print(\"Final WordSim shape:\", ws.shape)\n",
    "ws.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d861653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def similarity_scores_torch(lookup, ws_df):\n",
    "    \"\"\"\n",
    "    lookup: dict with keys {stoi, Wn}\n",
    "    ws_df: DataFrame with columns ['Word 1', 'Word 2', 'Human (mean)']\n",
    "    \"\"\"\n",
    "    stoi_local = lookup[\"stoi\"]\n",
    "    Wn = lookup[\"Wn\"]  # normalized embeddings\n",
    "\n",
    "    sims = []\n",
    "    gold = []\n",
    "    skipped = 0\n",
    "\n",
    "    for _, row in ws_df.iterrows():\n",
    "        w1 = row[\"Word 1\"]\n",
    "        w2 = row[\"Word 2\"]\n",
    "        score = row[\"Human (mean)\"]\n",
    "\n",
    "        if w1 not in stoi_local or w2 not in stoi_local:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        v1 = Wn[stoi_local[w1]]\n",
    "        v2 = Wn[stoi_local[w2]]\n",
    "\n",
    "        sim = torch.dot(v1, v2).item()  # cosine similarity\n",
    "        sims.append(sim)\n",
    "        gold.append(score)\n",
    "\n",
    "    return np.array(sims), np.array(gold), skipped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def similarity_scores_gensim(model, ws_df):\n",
    "    sims = []\n",
    "    gold = []\n",
    "    skipped = 0\n",
    "\n",
    "    for _, row in ws_df.iterrows():\n",
    "        w1 = row[\"Word 1\"]\n",
    "        w2 = row[\"Word 2\"]\n",
    "        score = row[\"Human (mean)\"]\n",
    "\n",
    "        if w1 not in model or w2 not in model:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        sim = model.similarity(w1, w2)\n",
    "        sims.append(sim)\n",
    "        gold.append(score)\n",
    "\n",
    "    return np.array(sims), np.array(gold), skipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18761449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>spearman</th>\n",
       "      <th>mse</th>\n",
       "      <th>skipped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skipgram</td>\n",
       "      <td>0.277</td>\n",
       "      <td>33.791</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skipgram (NEG)</td>\n",
       "      <td>0.042</td>\n",
       "      <td>33.911</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.189</td>\n",
       "      <td>35.915</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GloVe (gensim)</td>\n",
       "      <td>0.536</td>\n",
       "      <td>33.298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  spearman     mse  skipped\n",
       "0        Skipgram     0.277  33.791      259\n",
       "1  Skipgram (NEG)     0.042  33.911      259\n",
       "2           GloVe     0.189  35.915      259\n",
       "3  GloVe (gensim)     0.536  33.298        0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "results_similarity = []\n",
    "\n",
    "# Skipgram\n",
    "sims, gold, skipped = similarity_scores_torch(skipgram_lookup, ws)\n",
    "rho, _ = spearmanr(sims, gold)\n",
    "mse = np.mean((sims - gold) ** 2)\n",
    "results_similarity.append({\"model\": \"Skipgram\", \"spearman\": rho, \"mse\": mse, \"skipped\": skipped})\n",
    "\n",
    "# Skipgram NEG\n",
    "sims, gold, skipped = similarity_scores_torch(skipgram_neg_lookup, ws)\n",
    "rho, _ = spearmanr(sims, gold)\n",
    "mse = np.mean((sims - gold) ** 2)\n",
    "results_similarity.append({\"model\": \"Skipgram (NEG)\", \"spearman\": rho, \"mse\": mse, \"skipped\": skipped})\n",
    "\n",
    "# GloVe (your training)\n",
    "sims, gold, skipped = similarity_scores_torch(glove_lookup, ws)\n",
    "rho, _ = spearmanr(sims, gold)\n",
    "mse = np.mean((sims - gold) ** 2)\n",
    "results_similarity.append({\"model\": \"GloVe\", \"spearman\": rho, \"mse\": mse, \"skipped\": skipped})\n",
    "\n",
    "# GloVe (gensim baseline)\n",
    "sims, gold, skipped = similarity_scores_gensim(glove_gensim, ws)\n",
    "rho, _ = spearmanr(sims, gold)\n",
    "mse = np.mean((sims - gold) ** 2)\n",
    "results_similarity.append({\"model\": \"GloVe (gensim)\", \"spearman\": rho, \"mse\": mse, \"skipped\": skipped})\n",
    "\n",
    "sim_df = pd.DataFrame(results_similarity)\n",
    "sim_df[\"spearman\"] = sim_df[\"spearman\"].round(3)\n",
    "sim_df[\"mse\"] = sim_df[\"mse\"].round(3)\n",
    "\n",
    "sim_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f435d638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Window Size</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training time</th>\n",
       "      <th>Syntactic Accuracy</th>\n",
       "      <th>Semantic accuracy</th>\n",
       "      <th>Spearman</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word2Vec Skip-gram (full softmax)</td>\n",
       "      <td>2</td>\n",
       "      <td>4.6450</td>\n",
       "      <td>8.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word2Vec Skip-gram NEG</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7966</td>\n",
       "      <td>70.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.189</td>\n",
       "      <td>35.915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model  Window Size  Training Loss  \\\n",
       "0  Word2Vec Skip-gram (full softmax)            2         4.6450   \n",
       "1             Word2Vec Skip-gram NEG            2         0.7966   \n",
       "2                              GloVe            2         0.0455   \n",
       "\n",
       "   Training time  Syntactic Accuracy  Semantic accuracy  Spearman     MSE  \n",
       "0           8.65                 NaN                NaN       NaN     NaN  \n",
       "1          70.81                 NaN                NaN       NaN     NaN  \n",
       "2           0.92                 0.0                0.0     0.189  35.915  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "base = df.copy()\n",
    "\n",
    "\n",
    "keep = [\"model\", \"window\", \"training_time_sec\", \"final_loss\"]\n",
    "base = base[[c for c in keep if c in base.columns]].copy()\n",
    "\n",
    "\n",
    "base = base.rename(columns={\n",
    "    \"model\": \"Model\",\n",
    "    \"window\": \"Window Size\",\n",
    "    \"training_time_sec\": \"Training time\",\n",
    "    \"final_loss\": \"Training Loss\",\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "ana = analogy_df.copy()\n",
    "ana = ana[[\"model\", \"syntactic_acc_%\", \"semantic_acc_%\"]].rename(columns={\n",
    "    \"model\": \"Model\",\n",
    "    \"syntactic_acc_%\": \"Syntactic Accuracy\",\n",
    "    \"semantic_acc_%\": \"Semantic accuracy\",\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "sim = sim_df.copy()\n",
    "sim = sim[[\"model\", \"spearman\", \"mse\"]].rename(columns={\n",
    "    \"model\": \"Model\",\n",
    "    \"spearman\": \"Spearman\",\n",
    "    \"mse\": \"MSE\",\n",
    "})\n",
    "\n",
    "final_table = base.merge(ana, on=\"Model\", how=\"left\").merge(sim, on=\"Model\", how=\"left\")\n",
    "\n",
    "\n",
    "for col in [\"Training Loss\", \"Training time\", \"Spearman\", \"MSE\"]:\n",
    "    if col in final_table.columns:\n",
    "        final_table[col] = pd.to_numeric(final_table[col], errors=\"coerce\")\n",
    "\n",
    "final_table[\"Training Loss\"] = final_table[\"Training Loss\"].round(4)\n",
    "final_table[\"Training time\"] = final_table[\"Training time\"].round(2)\n",
    "final_table[\"Spearman\"] = final_table[\"Spearman\"].round(3)\n",
    "final_table[\"MSE\"] = final_table[\"MSE\"].round(3)\n",
    "\n",
    "\n",
    "final_table[\"Syntactic Accuracy\"] = final_table[\"Syntactic Accuracy\"].round(2)\n",
    "final_table[\"Semantic accuracy\"] = final_table[\"Semantic accuracy\"].round(2)\n",
    "\n",
    "\n",
    "final_cols = [\n",
    "    \"Model\",\n",
    "    \"Window Size\",\n",
    "    \"Training Loss\",\n",
    "    \"Training time\",\n",
    "    \"Syntactic Accuracy\",\n",
    "    \"Semantic accuracy\",\n",
    "    \"Spearman\",\n",
    "    \"MSE\"\n",
    "]\n",
    "final_table = final_table[[c for c in final_cols if c in final_table.columns]]\n",
    "\n",
    "final_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e05bb849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: task2_final_table.csv\n"
     ]
    }
   ],
   "source": [
    "final_table.to_csv(\"task2_final_table.csv\", index=False)\n",
    "print(\"Saved: task2_final_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec185fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Samir\n",
      "[nltk_data]     Pokharel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexts: 4211\n",
      "Example: the fulton county grand jury said friday an investigation of atlanta's recent primary election produced no evidence '' that any irregularities took place\n"
     ]
    }
   ],
   "source": [
    "# Task 3 - Step 1: Build searchable contexts (sentences)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "# Use Brown news sentences (already tokenized)\n",
    "raw_sents = brown.sents(categories=\"news\")\n",
    "\n",
    "def clean_token(tok: str):\n",
    "    tok = tok.lower()\n",
    "    tok = re.sub(r\"[^a-z']+\", \"\", tok)\n",
    "    return tok\n",
    "\n",
    "# Clean sentences and also keep original text for display\n",
    "contexts_tokens = []\n",
    "contexts_text = []\n",
    "\n",
    "for sent in raw_sents:\n",
    "    cleaned = [clean_token(t) for t in sent]\n",
    "    cleaned = [t for t in cleaned if t]\n",
    "    if len(cleaned) >= 5:  # keep non-trivial sentences\n",
    "        contexts_tokens.append(cleaned)\n",
    "        contexts_text.append(\" \".join(cleaned))\n",
    "\n",
    "print(\"Contexts:\", len(contexts_text))\n",
    "print(\"Example:\", contexts_text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2397dc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs shape: torch.Size([4211, 100])\n",
      "Using model: skipgram_neg\n"
     ]
    }
   ],
   "source": [
    "# Task 3 - Step 2: Choose embedding + precompute sentence vectors\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---- Choose which embeddings to use ----\n",
    "# Options: \"skipgram\", \"skipgram_neg\", \"glove\"\n",
    "EMBED_MODEL = \"skipgram_neg\"\n",
    "\n",
    "# Get embedding matrix from your trained models\n",
    "if EMBED_MODEL == \"skipgram\":\n",
    "    W = model.in_embed.weight.detach().cpu()\n",
    "elif EMBED_MODEL == \"skipgram_neg\":\n",
    "    W = model_neg.in_embed.weight.detach().cpu()\n",
    "elif EMBED_MODEL == \"glove\":\n",
    "    W = glove_W.detach().cpu()  # from your GloVe training\n",
    "else:\n",
    "    raise ValueError(\"Invalid EMBED_MODEL\")\n",
    "\n",
    "# Normalize word embeddings so dot-product == cosine similarity\n",
    "Wn = F.normalize(W.float(), p=2, dim=1)\n",
    "\n",
    "# ---- Helpers ----\n",
    "UNK_ID = stoi.get(\"<unk>\", 0)\n",
    "\n",
    "def sent_to_vec(tokens):\n",
    "    ids = [stoi.get(w, UNK_ID) for w in tokens]\n",
    "    vecs = Wn[ids]  # (L, D)\n",
    "    v = vecs.mean(dim=0)\n",
    "    v = F.normalize(v, p=2, dim=0)\n",
    "    return v\n",
    "\n",
    "# ---- Precompute context vectors (CPU tensor) ----\n",
    "# (this makes search fast)\n",
    "context_vecs = torch.stack([sent_to_vec(toks) for toks in contexts_tokens])  # (N, D)\n",
    "\n",
    "print(\"context_vecs shape:\", context_vecs.shape)\n",
    "print(\"Using model:\", EMBED_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48a7e2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: oil prices\n",
      "01. (0.591) union oil co of california tuesday offered million in debentures to the public through a group of underwriters headed by dillon read co to raise money to retire a similar amount held by gulf oil corp\n",
      "02. (0.582) they provided a social security system which covered all their african employes\n",
      "03. (0.574) the company began operation in with hardware and oil mill supplies\n",
      "04. (0.574) american stock exchange prices enjoyed a fairly solid rise but here also trading dwindled\n",
      "05. (0.571) higher toll rates also are helping boost revenues\n",
      "06. (0.566) the higher price supports provided by the new legislation together with rising prices for farm products are pushing up farm income making it possible for farmers to afford the new machinery\n",
      "07. (0.562) the british coal industry is unprofitable has large coal stocks it can't sell\n",
      "08. (0.562) within a year without reducing wages underwood's production costs were cut one third prices were slashed\n",
      "09. (0.553) a large wellstocked library surrounded in a county by smaller ones may feel that the demands on its resources are likely to be too great\n",
      "10. (0.553) he cited as an example how the american camera industry has been able to meet successfully the competition of japan despite lower japanese labor costs by improving its production knowhow and technology\n"
     ]
    }
   ],
   "source": [
    "# Task 3 - Step 3: Dot-product search (Top 10)\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def tokenize_query(q: str):\n",
    "    q = q.lower()\n",
    "    return re.findall(r\"[a-z]+(?:'[a-z]+)?\", q)\n",
    "\n",
    "def query_to_vec(query: str):\n",
    "    toks = tokenize_query(query)\n",
    "    if len(toks) == 0:\n",
    "        return None\n",
    "    ids = [stoi.get(w, UNK_ID) for w in toks]\n",
    "    v = Wn[ids].mean(dim=0)\n",
    "    v = F.normalize(v, p=2, dim=0)\n",
    "    return v\n",
    "\n",
    "def search_topk(query: str, k=10):\n",
    "    qv = query_to_vec(query)\n",
    "    if qv is None:\n",
    "        return []\n",
    "\n",
    "    # Dot product (cosine because normalized)\n",
    "    scores = torch.mv(context_vecs, qv)  # (N,)\n",
    "    vals, idxs = torch.topk(scores, k=k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(vals.tolist(), idxs.tolist()):\n",
    "        results.append({\n",
    "            \"score\": float(score),\n",
    "            \"text\": contexts_text[idx]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ---- Quick test ----\n",
    "test_query = \"oil prices\"\n",
    "hits = search_topk(test_query, k=10)\n",
    "\n",
    "print(\"Query:\", test_query)\n",
    "for i, h in enumerate(hits, 1):\n",
    "    print(f\"{i:02d}. ({h['score']:.3f}) {h['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbfcfd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved search_data.pt with models: skipgram, neg, glove\n",
      "Context vecs shape: torch.Size([4211, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_Wn_from_weight(weight_cpu):\n",
    "    return F.normalize(weight_cpu.float(), p=2, dim=1)\n",
    "\n",
    "def sent_to_vec_with_Wn(tokens, Wn):\n",
    "    ids = [stoi.get(w, UNK_ID) for w in tokens]\n",
    "    v = Wn[ids].mean(dim=0)\n",
    "    return F.normalize(v, p=2, dim=0)\n",
    "\n",
    "# Build normalized word vectors for each model\n",
    "Wn_skipgram = build_Wn_from_weight(model.in_embed.weight.detach().cpu())\n",
    "Wn_neg      = build_Wn_from_weight(model_neg.in_embed.weight.detach().cpu())\n",
    "Wn_glove    = build_Wn_from_weight(glove_W.detach().cpu())\n",
    "\n",
    "# Precompute context vectors for each model\n",
    "context_vecs_skipgram = torch.stack([sent_to_vec_with_Wn(toks, Wn_skipgram) for toks in contexts_tokens])\n",
    "context_vecs_neg      = torch.stack([sent_to_vec_with_Wn(toks, Wn_neg)      for toks in contexts_tokens])\n",
    "context_vecs_glove    = torch.stack([sent_to_vec_with_Wn(toks, Wn_glove)    for toks in contexts_tokens])\n",
    "\n",
    "torch.save({\n",
    "    \"stoi\": stoi,\n",
    "    \"UNK_ID\": UNK_ID,\n",
    "    \"contexts_text\": contexts_text,\n",
    "\n",
    "    \"models\": {\n",
    "        \"skipgram\": {\"Wn\": Wn_skipgram, \"context_vecs\": context_vecs_skipgram},\n",
    "        \"neg\":      {\"Wn\": Wn_neg,      \"context_vecs\": context_vecs_neg},\n",
    "        \"glove\":    {\"Wn\": Wn_glove,    \"context_vecs\": context_vecs_glove},\n",
    "    }\n",
    "}, \"search_data.pt\")\n",
    "\n",
    "print(\"Saved search_data.pt with models: skipgram, neg, glove\")\n",
    "print(\"Context vecs shape:\", context_vecs_skipgram.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ae73197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from flask import Flask, request, render_template_string\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DATA_PATH = \"search_data.pt\"\n",
    "app = Flask(__name__)\n",
    "\n",
    "TEMPLATE = \"\"\"\n",
    "<!doctype html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>NLP Search Engine</title>\n",
    "    <style>\n",
    "      body { font-family: Arial, sans-serif; margin: 40px; }\n",
    "      input[type=text] { width: 420px; padding: 10px; font-size: 16px; }\n",
    "      select { padding: 10px; font-size: 16px; margin-left: 8px; }\n",
    "      button { padding: 10px 16px; font-size: 16px; margin-left: 8px; }\n",
    "      .result { margin-top: 14px; padding: 10px; border: 1px solid #ddd; border-radius: 8px; }\n",
    "      .meta { color: #555; font-size: 14px; margin-top: 6px; }\n",
    "      .score { color: #555; font-size: 14px; }\n",
    "    </style>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h2>Search Similar Context (Dot Product)</h2>\n",
    "\n",
    "    <form method=\"GET\" action=\"/\">\n",
    "      <input type=\"text\" name=\"q\" placeholder=\"Type your query...\" value=\"{{q|default('')}}\" />\n",
    "\n",
    "      <select name=\"m\">\n",
    "        {% for key, label in model_options %}\n",
    "          <option value=\"{{key}}\" {% if key == model_selected %}selected{% endif %}>{{label}}</option>\n",
    "        {% endfor %}\n",
    "      </select>\n",
    "\n",
    "      <button type=\"submit\">Search</button>\n",
    "    </form>\n",
    "\n",
    "    {% if results is not none %}\n",
    "      <div class=\"meta\">\n",
    "        Model: <b>{{ model_selected }}</b>\n",
    "      </div>\n",
    "\n",
    "      <h3>Top 10 Results</h3>\n",
    "      {% if results|length == 0 %}\n",
    "        <p>No results (empty query).</p>\n",
    "      {% endif %}\n",
    "\n",
    "      {% for r in results %}\n",
    "        <div class=\"result\">\n",
    "          <div class=\"score\">Score: {{ \"%.3f\"|format(r.score) }}</div>\n",
    "          <div>{{ r.text }}</div>\n",
    "        </div>\n",
    "      {% endfor %}\n",
    "    {% endif %}\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_query(q: str):\n",
    "    q = q.lower()\n",
    "    return re.findall(r\"[a-z]+(?:'[a-z]+)?\", q)\n",
    "\n",
    "DATA = torch.load(DATA_PATH, map_location=\"cpu\")\n",
    "stoi = DATA[\"stoi\"]\n",
    "UNK_ID = DATA[\"UNK_ID\"]\n",
    "contexts_text = DATA[\"contexts_text\"]\n",
    "MODEL_STORE = DATA[\"models\"]  # dict: skipgram/neg/glove\n",
    "\n",
    "MODEL_OPTIONS = [\n",
    "    (\"skipgram\", \"Skipgram\"),\n",
    "    (\"neg\", \"Skipgram (NEG)\"),\n",
    "    (\"glove\", \"GloVe\"),\n",
    "]\n",
    "\n",
    "def query_to_vec(query: str, Wn):\n",
    "    toks = tokenize_query(query)\n",
    "    if len(toks) == 0:\n",
    "        return None\n",
    "    ids = [stoi.get(w, UNK_ID) for w in toks]\n",
    "    v = Wn[ids].mean(dim=0)\n",
    "    v = F.normalize(v, p=2, dim=0)\n",
    "    return v\n",
    "\n",
    "def search_topk(query: str, model_key: str, k=10):\n",
    "    model_key = model_key if model_key in MODEL_STORE else \"neg\"\n",
    "    Wn = MODEL_STORE[model_key][\"Wn\"]\n",
    "    context_vecs = MODEL_STORE[model_key][\"context_vecs\"]\n",
    "\n",
    "    qv = query_to_vec(query, Wn)\n",
    "    if qv is None:\n",
    "        return []\n",
    "\n",
    "    scores = torch.mv(context_vecs, qv)\n",
    "    vals, idxs = torch.topk(scores, k=min(k, scores.numel()))\n",
    "\n",
    "    out = []\n",
    "    for s, idx in zip(vals.tolist(), idxs.tolist()):\n",
    "        out.append(type(\"R\", (), {\"score\": float(s), \"text\": contexts_text[idx]}))\n",
    "    return out\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def index():\n",
    "    q = request.args.get(\"q\", \"\").strip()\n",
    "    m = request.args.get(\"m\", \"neg\").strip()\n",
    "\n",
    "    results = None\n",
    "    if q != \"\":\n",
    "        results = search_topk(q, model_key=m, k=10)\n",
    "    elif \"q\" in request.args:\n",
    "        results = []\n",
    "\n",
    "    return render_template_string(\n",
    "        TEMPLATE,\n",
    "        q=q,\n",
    "        results=results,\n",
    "        model_selected=m if m in MODEL_STORE else \"neg\",\n",
    "        model_options=MODEL_OPTIONS\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000, debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbead1f",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2556ba",
   "metadata": {},
   "source": [
    "As observed, for a window size of 2, the Word2Vec Skip-gram (full softmax) model recorded the highest training loss (4.6450), indicating difficulty in learning meaningful representations using full softmax on a relatively small corpus. In contrast, Skip-gram with Negative Sampling achieved a significantly lower loss of 0.7966, while GloVe obtained the lowest training loss (0.0455), showing faster and more stable convergence.\n",
    "\n",
    "In terms of training time, Skip-gram with Negative Sampling was the slowest model (70.81s) due to repeated negative sampling operations. Skip-gram (full softmax) required 8.65s, while GloVe was the fastest, completing training in 0.92s, even when accounting for co-occurrence matrix construction (total time ≈ 1.24s).\n",
    "\n",
    "For word analogy evaluation, all three models trained from scratch (Skip-gram, Skip-gram NEG, and GloVe) achieved 0% syntactic and semantic accuracy. This result was expected and can be attributed to the limited size of the Brown news corpus, small window size, and relatively low embedding dimension. These limitations prevent the models from learning robust relational patterns required for analogy tasks.\n",
    "\n",
    "In contrast, pre-trained GloVe (Gensim) significantly outperformed the custom-trained models. For word similarity evaluation (WordSim353), GloVe (Gensim) achieved the highest Spearman correlation (0.536) with no skipped word pairs, indicating strong alignment with human judgment. The scratch-trained models showed much lower correlations (Skip-gram: 0.277, Skip-gram NEG: 0.042, GloVe: 0.189) and skipped many word pairs due to vocabulary mismatch, suggesting weak semantic consistency.\n",
    "\n",
    "Overall, these results demonstrate that embedding models trained from scratch on small corpora perform poorly when evaluated on intrinsic benchmarks. However, with larger datasets, higher embedding dimensions, better hyperparameter tuning, and longer training, the performance of these models can be significantly improved. Pre-trained embeddings remain a strong baseline for semantic tasks under limited data conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bebf6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Natural Language Processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
